{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTS**"
      ],
      "metadata": {
        "id": "7NPj4hr-qQi_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "444dTQzDonsX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/kaggle/input/2025-sep-dl-gen-ai-project/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/2025-sep-dl-gen-ai-project/test.csv')\n",
        "sample = pd.read_csv(\"/kaggle/input/2025-sep-dl-gen-ai-project/sample_submission.csv\")"
      ],
      "metadata": {
        "id": "uxCSbagQoqQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key = \"1cae1eb0b3009c258573b649b577124df891befe\" , relogin=True)\n",
        "\n",
        "wandb.init(\n",
        "    project=\"23f1001420-t32025\",\n",
        "    name=\"roberta_trainer\",\n",
        "    config={\n",
        "        \"model_name\": \"roberta-base\",\n",
        "        \"batch_size\": 16,\n",
        "        \"lr\": 2e-5,\n",
        "        \"epochs\": 3,\n",
        "        \"max_len\": 128\n",
        "    }\n",
        ")\n",
        "config = wandb.config"
      ],
      "metadata": {
        "id": "s0JGf-RroqMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Dataset Class**\n",
        "\n",
        "This block defines a PyTorch Dataset class that tokenizes text inputs and returns input IDs, attention masks, and corresponding multi-label emotion targets during training. It also supports inference by returning only the tokenized inputs when is_train=False."
      ],
      "metadata": {
        "id": "6OeCPk1CDfml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_cols = [\"anger\",\"fear\",\"joy\",\"sadness\",\"surprise\"]\n",
        "\n",
        "X_train, X_val = train_test_split(train, test_size=0.1, random_state=42, shuffle=True)\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=128, is_train=True):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        text = str(row[\"text\"])\n",
        "\n",
        "        enc = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        if self.is_train:\n",
        "            labels = torch.tensor(row[label_cols].values.astype('float32'))\n",
        "            return enc.input_ids.squeeze(), enc.attention_mask.squeeze(), labels\n",
        "        else:\n",
        "            return enc.input_ids.squeeze(), enc.attention_mask.squeeze()\n"
      ],
      "metadata": {
        "id": "YQyHjALhoqKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom RoBERTa-Based Emotion Classifier and DataLoaders**\n",
        "\n",
        "This section defines a PyTorch nn.Module for multi-label emotion classification using a pre-trained RoBERTa encoder, followed by a dropout layer and a linear classifier. It also creates EmotionDataset instances for training, validation, and test sets, and wraps them in PyTorch DataLoaders for efficient batching during training and evaluation."
      ],
      "metadata": {
        "id": "XkamTpdJDnJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self, model_name=\"roberta-base\", dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(self.encoder.config.hidden_size, 5)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = out.last_hidden_state[:,0]  # CLS token\n",
        "        x = self.dropout(pooled)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "train_ds = EmotionDataset(X_train, tokenizer, is_train=True)\n",
        "val_ds   = EmotionDataset(X_val, tokenizer, is_train=True)\n",
        "test_ds  = EmotionDataset(test, tokenizer, is_train=False)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "id": "aO_0e2WVoqHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training and Validation Loops**\n",
        "\n",
        "This section defines the training loop using BCEWithLogitsLoss for multi-label classification, with an AdamW optimizer and a linear learning rate scheduler. During training, batch losses are logged to Weights & Biases, and after each epoch, the model is evaluated on the validation set using Macro F1-score to monitor performance."
      ],
      "metadata": {
        "id": "gw_Ju90sDwvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = EmotionClassifier().to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
        "epochs = config.epochs\n",
        "\n",
        "total_steps = len(train_loader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "def train_model():\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader):\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            wandb.log({\"batch_loss\": loss.item()})\n",
        "\n",
        "        epoch_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        wandb.log({\"train_epoch_loss\": epoch_loss})\n",
        "\n",
        "        validate(epoch)\n",
        "\n",
        "def validate(epoch):\n",
        "    model.eval()\n",
        "    preds_list = []\n",
        "    true_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "            preds_list.append(preds)\n",
        "            true_list.append(labels.cpu().numpy())\n",
        "\n",
        "    preds_list = np.vstack(preds_list)\n",
        "    true_list = np.vstack(true_list)\n",
        "\n",
        "    # Macro F1 calculation across 5 labels\n",
        "    from sklearn.metrics import f1_score\n",
        "    binary_preds = (preds_list > 0.5).astype(int)\n",
        "    f1 = f1_score(true_list, binary_preds, average=\"macro\")\n",
        "    print(\"Validation Macro F1:\", f1)\n",
        "    wandb.log({\"val_macro_f1\": f1})\n"
      ],
      "metadata": {
        "id": "AhBjzO48oqEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model()"
      ],
      "metadata": {
        "id": "UO0K8Af8oqCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "gawbC8joop_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saving and Uploading the Trained Model**"
      ],
      "metadata": {
        "id": "Ibo90mWFD60L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"roberta_emotion_model\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(save_dir, \"model.pth\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(\"Saved:\", model_path)\n",
        "\n",
        "user = \"somya2611\"\n",
        "upload_handle = f\"{user}/roberta-emotion/pyTorch/v1\"\n",
        "\n",
        "kagglehub.model_upload(upload_handle, \"roberta_emotion_model\")\n",
        "\n",
        "print(\"Model uploaded to:\", upload_handle)"
      ],
      "metadata": {
        "id": "5SuaPfnhop8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Downloading the Saved Model from Kaggle**"
      ],
      "metadata": {
        "id": "JNgkens1EAFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "user = \"somya2611\"\n",
        "download_handle = f\"{user}/roberta-emotion/pyTorch/v1\"\n",
        "\n",
        "model_folder = kagglehub.model_download(download_handle)\n",
        "print(\"Downloaded model folder:\", model_folder)"
      ],
      "metadata": {
        "id": "oYzdzAlJop6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_model = EmotionClassifier().to(device)\n",
        "state_dict_path = f\"{model_folder}/model.pth\"\n",
        "\n",
        "inference_model.load_state_dict(torch.load(state_dict_path, map_location=device))\n",
        "inference_model.eval()\n",
        "\n",
        "print(\"Model loaded for inference!\")"
      ],
      "metadata": {
        "id": "lV7tA3R4op3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Running Inference and Generating Submission File**"
      ],
      "metadata": {
        "id": "kfPdOgd4EM8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "test_ds  = EmotionDataset(test, tokenizer, is_train=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=16, shuffle=False)\n",
        "\n",
        "\n",
        "final_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask = [b.to(device) for b in batch]\n",
        "        outputs = inference_model(input_ids, attention_mask)\n",
        "        probs = torch.sigmoid(outputs).cpu().numpy()\n",
        "        preds = (probs > 0.5).astype(int)\n",
        "        final_preds.append(preds)\n",
        "\n",
        "final_preds = np.vstack(final_preds)\n",
        "\n",
        "\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test[\"id\"],\n",
        "    \"anger\": final_preds[:, 0],\n",
        "    \"fear\": final_preds[:, 1],\n",
        "    \"joy\": final_preds[:, 2],\n",
        "    \"sadness\": final_preds[:, 3],\n",
        "    \"surprise\": final_preds[:, 4],\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "submission.head()\n"
      ],
      "metadata": {
        "id": "1DNGiQ2Sop1F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}