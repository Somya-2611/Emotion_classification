{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTS**"
      ],
      "metadata": {
        "id": "e7wwfgAAiqkC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFyhIpLHh_rD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    set_seed\n",
        ")\n",
        "\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/kaggle/input/2025-sep-dl-gen-ai-project/train.csv')\n",
        "test_df = pd.read_csv('/kaggle/input/2025-sep-dl-gen-ai-project/test.csv')"
      ],
      "metadata": {
        "id": "p-e_FgzWjQUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key = \"1cae1eb0b3009c258573b649b577124df891befe\" , relogin=True)"
      ],
      "metadata": {
        "id": "57nstnGSiHsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparing Dataset**\n",
        "This block converts the emotion label columns into integer format and splits the dataset into training and validation sets. It then creates HuggingFace Dataset objects from the pandas DataFrames and adds a labels field for each example, which stores all five emotion labels as a list. These processed datasets are later used for model training and evaluation"
      ],
      "metadata": {
        "id": "15rFzbHfjhjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_cols = ['anger','fear','joy','sadness','surprise']\n",
        "train_df[label_cols] = train_df[label_cols].astype(int)\n",
        "\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=SEED, shuffle=True)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df   = val_df.reset_index(drop=True)\n",
        "\n",
        "hf_train = Dataset.from_pandas(train_df[['id','text'] + label_cols])\n",
        "hf_val   = Dataset.from_pandas(val_df[['id','text'] + label_cols])\n",
        "hf_test  = Dataset.from_pandas(test_df[['id','text']])\n",
        "\n",
        "def add_labels(example):\n",
        "    example[\"labels\"] = [float(example[c]) for c in label_cols]\n",
        "    return example\n",
        "\n",
        "hf_train = hf_train.map(add_labels)\n",
        "hf_val   = hf_val.map(add_labels)"
      ],
      "metadata": {
        "id": "OeZUqljViHpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization and Data Preparation for DeBERTa**\n",
        "\n",
        "This step initializes the DeBERTa tokenizer and applies it to the train, validation, and test datasets. The text is converted into token IDs with a fixed maximum length, while labels are removed from the HuggingFace datasets since they will be handled separately. A DataCollatorWithPadding is also used to dynamically pad batches during training."
      ],
      "metadata": {
        "id": "_fn5-VaokAnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "MAX_LENGTH = 256\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, padding=False, max_length=MAX_LENGTH)\n",
        "\n",
        "hf_train = hf_train.map(tokenize, batched=True)\n",
        "hf_val   = hf_val.map(tokenize, batched=True)\n",
        "hf_test  = hf_test.map(tokenize, batched=True)\n",
        "\n",
        "\n",
        "hf_train = hf_train.remove_columns(label_cols)\n",
        "hf_val   = hf_val.remove_columns(label_cols)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "_ImBLLTEjJKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Initialization and Evaluation Metrics**\n",
        "\n",
        "This section loads the DeBERTa model for multi-label emotion classification and defines the evaluation metrics used during training. Predictions are converted into probabilities using a sigmoid activation, thresholded at 0.5, and evaluated using Macro F1-score along with class-wise F1-scores for all five emotions."
      ],
      "metadata": {
        "id": "sPZSmiDwkpGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(label_cols),\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    probs = torch.sigmoid(torch.tensor(logits))\n",
        "    preds = (probs > 0.5).int().numpy()\n",
        "    labels = labels.astype(int)\n",
        "    macro_f1 = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
        "    per_label_f1 = f1_score(labels, preds, average=None, zero_division=0).tolist()\n",
        "    return {\n",
        "        \"macro_f1\": macro_f1,\n",
        "        \"f1_anger\": per_label_f1[0],\n",
        "        \"f1_fear\": per_label_f1[1],\n",
        "        \"f1_joy\": per_label_f1[2],\n",
        "        \"f1_sadness\": per_label_f1[3],\n",
        "        \"f1_surprise\": per_label_f1[4],\n",
        "    }"
      ],
      "metadata": {
        "id": "YIA3iY0GiHm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Configuration and Trainer Setup**\n",
        "\n",
        "This section defines the training hyperparameters such as batch size, learning rate, number of epochs, and evaluation strategy using HuggingFaceâ€™s TrainingArguments. We also initialize a Weights & Biases run for experiment tracking and create a Trainer object that combines the model, datasets, tokenizer, data collator, and evaluation metrics for training and validation."
      ],
      "metadata": {
        "id": "CjM8a1FRk1et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./deberta_v3_base_emotion\",\n",
        "\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=1,\n",
        "\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=3,\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\",\n",
        "\n",
        "    save_total_limit=2,\n",
        "    fp16=True if torch.cuda.is_available() else False,\n",
        "    seed=SEED,\n",
        "\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"DeBERTa-v3-emotion\",\n",
        "\n",
        ")\n",
        "\n",
        "wandb.init(project=\"23f1001420-t32025\", name=\"deberta-v3-base\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=hf_train,\n",
        "    eval_dataset=hf_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "Q8bR0_QCiHkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "6pHE4mV4iRJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "7HiWemWtiRG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saving and Uploading the Trained Model**\n",
        "\n",
        "This step saves the fine-tuned DeBERTa model and tokenizer into a local directory. After saving, the complete model folder is uploaded to Kaggle Models using kagglehub, allowing easy reuse for inference or future experiments."
      ],
      "metadata": {
        "id": "FT2J1YEhlAf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"deberta_full_model\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save HF model\n",
        "trainer.save_model(save_dir)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "print(\"Saved full model folder:\", os.listdir(save_dir))\n",
        "user = \"somya2611\"\n",
        "Deberta_handle = f\"{user}/DebertaFull/pyTorch/v1\"\n",
        "\n",
        "kagglehub.model_upload(Deberta_handle, \"deberta_full_model\")"
      ],
      "metadata": {
        "id": "YRbEIjNXiREO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Downloading the Saved Model from Kaggle**\n",
        "\n",
        "This section sets the appropriate compute device (CPU or GPU) and downloads the previously uploaded model from Kaggle Models using `kagglehub`. Once downloaded, the contents of the model folder are listed to confirm successful retrieval.\n"
      ],
      "metadata": {
        "id": "AG-UBRfjlIs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "user = \"somya2611\"\n",
        "Deberta_handle = f\"{user}/DebertaFull/pyTorch/v1\"\n",
        "\n",
        "model_folder = kagglehub.model_download(Deberta_handle)\n",
        "print(\"Files downloaded:\", os.listdir(model_folder))\n"
      ],
      "metadata": {
        "id": "NoHW8F3RiRBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading the Fine-Tuned Model for Inference**\n",
        "\n",
        "This section loads the tokenizer and the fine-tuned DeBERTa model from the saved model folder downloaded from Kaggle. The model is moved to the appropriate device (CPU/GPU) and set to evaluation mode so it can generate predictions without performing any training steps."
      ],
      "metadata": {
        "id": "KHvSlNnPlrcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_folder)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_folder,\n",
        "    problem_type=\"multi_label_classification\"\n",
        ").to(device)\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "RNby_OIKic9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Running Inference and Generating Submission File**\n",
        "\n",
        "In this step, a lightweight Trainer is created for inference, and predictions are generated on the test dataset. The model outputs logits, which are converted to probabilities and then thresholded to obtain binary emotion labels. Finally, a submission file is created in the required format and saved as submission.csv."
      ],
      "metadata": {
        "id": "t-PIS5B7lx9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "preds = inference_trainer.predict(hf_test)\n",
        "logits = preds.predictions\n",
        "probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
        "binary_preds = (probs > 0.5).astype(int)\n",
        "\n",
        "\n",
        "submission = pd.DataFrame(binary_preds, columns=label_cols)\n",
        "submission.insert(0, \"id\", test_df[\"id\"].values)\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"Saved submission.csv\")\n",
        "submission.head()\n"
      ],
      "metadata": {
        "id": "kciny6yuic6L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}