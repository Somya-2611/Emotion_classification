{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTS**"
      ],
      "metadata": {
        "id": "4d1-41JZE8ub"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JaNbBR7Ei1O"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(Config.SEED)\n",
        "print(f\"Using device: {Config.DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/kaggle/input/2025-sep-dl-gen-ai-project/train.csv')\n",
        "test_df = pd.read_csv('/kaggle/input/2025-sep-dl-gen-ai-project/test.csv')"
      ],
      "metadata": {
        "id": "AWsbj4qOGXBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuration Class**\n",
        "\n",
        "This block defines a Config class containing all model, training, and augmentation hyperparameters for easy management."
      ],
      "metadata": {
        "id": "ofsKcsaPFZ7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    # Model architecture\n",
        "    VOCAB_SIZE = 30000\n",
        "    EMBEDDING_DIM = 400\n",
        "    HIDDEN_DIM = 512\n",
        "    NUM_LAYERS = 3\n",
        "    NUM_LABELS = 5\n",
        "    DROPOUT = 0.4\n",
        "    BIDIRECTIONAL = True\n",
        "\n",
        "    # Training settings\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 20\n",
        "    LEARNING_RATE = 0.0015\n",
        "    MAX_LENGTH = 150\n",
        "    WEIGHT_DECAY = 1e-5\n",
        "\n",
        "    # Augmentation\n",
        "    USE_AUGMENTATION = True\n",
        "    AUG_PROBABILITY = 0.3\n",
        "\n",
        "    SEED = 42\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    EMOTION_COLS = ['anger', 'fear', 'joy', 'sadness', 'surprise']"
      ],
      "metadata": {
        "id": "suzFD-ihEjnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key = \"1cae1eb0b3009c258573b649b577124df891befe\" , relogin=True)\n",
        "\n",
        "wandb.init(\n",
        "    project=\"23f1001420-t32025\",\n",
        "    name=\"scratch_emotion_classifier\",\n",
        "    config={\n",
        "        \"vocab_size\": Config.VOCAB_SIZE,\n",
        "        \"embedding_dim\": Config.EMBEDDING_DIM,\n",
        "        \"hidden_dim\": Config.HIDDEN_DIM,\n",
        "        \"num_layers\": Config.NUM_LAYERS,\n",
        "        \"num_labels\": Config.NUM_LABELS,\n",
        "        \"dropout\": Config.DROPOUT,\n",
        "        \"bidirectional\": Config.BIDIRECTIONAL,\n",
        "\n",
        "        \"batch_size\": Config.BATCH_SIZE,\n",
        "        \"epochs\": Config.EPOCHS,\n",
        "        \"learning_rate\": Config.LEARNING_RATE,\n",
        "        \"max_length\": Config.MAX_LENGTH,\n",
        "        \"weight_decay\": Config.WEIGHT_DECAY,\n",
        "\n",
        "        \"augmentation\": Config.USE_AUGMENTATION,\n",
        "        \"aug_prob\": Config.AUG_PROBABILITY,\n",
        "\n",
        "        \"seed\": Config.SEED,\n",
        "    }\n",
        ")\n",
        "\n",
        "config = wandb.config\n"
      ],
      "metadata": {
        "id": "jBteEmsLEjlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TEXT PREPROCESSING WITH AUGMENTATION**"
      ],
      "metadata": {
        "id": "qObfX7XMFnos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedTextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
        "        self.vocab_size = 2\n",
        "\n",
        "        # Emotion-specific keywords for better understanding\n",
        "        self.emotion_keywords = {\n",
        "            'anger': ['angry', 'mad', 'furious', 'annoyed', 'irritated', 'rage'],\n",
        "            'fear': ['scared', 'afraid', 'terrified', 'anxious', 'worried', 'nervous'],\n",
        "            'joy': ['happy', 'joyful', 'excited', 'glad', 'delighted', 'cheerful'],\n",
        "            'sadness': ['sad', 'depressed', 'unhappy', 'miserable', 'heartbroken', 'crying'],\n",
        "            'surprise': ['surprised', 'shocked', 'amazed', 'astonished', 'unexpected']\n",
        "        }\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Enhanced text cleaning\"\"\"\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # Preserve important punctuation\n",
        "        text = re.sub(r'!+', ' ! ', text)\n",
        "        text = re.sub(r'\\?+', ' ? ', text)\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "\n",
        "        # Handle negations\n",
        "        text = re.sub(r\"n't\", \" not\", text)\n",
        "        text = re.sub(r\"'m\", \" am\", text)\n",
        "        text = re.sub(r\"'re\", \" are\", text)\n",
        "        text = re.sub(r\"'ve\", \" have\", text)\n",
        "\n",
        "        # Remove mentions but keep hashtag content\n",
        "        text = re.sub(r'@\\w+', '', text)\n",
        "        text = re.sub(r'#', '', text)\n",
        "\n",
        "        # Keep letters, numbers, and some punctuation\n",
        "        text = re.sub(r'[^a-z0-9\\s.,!?\\'-]', '', text)\n",
        "\n",
        "        # Handle repeated characters (e.g., 'soooo' -> 'so')\n",
        "        text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Enhanced tokenization\"\"\"\n",
        "        return text.split()\n",
        "\n",
        "    def augment_text(self, text):\n",
        "        \"\"\"Simple augmentation: synonym replacement\"\"\"\n",
        "        words = text.split()\n",
        "        if len(words) < 3 or np.random.random() > Config.AUG_PROBABILITY:\n",
        "            return text\n",
        "\n",
        "        # Simple word dropout\n",
        "        if np.random.random() < 0.3:\n",
        "            idx = np.random.randint(0, len(words))\n",
        "            words.pop(idx)\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def build_vocab(self, texts, max_vocab_size=30000):\n",
        "        \"\"\"Build vocabulary from texts\"\"\"\n",
        "        print(\"Building vocabulary...\")\n",
        "        word_freq = Counter()\n",
        "\n",
        "        for text in tqdm(texts):\n",
        "            cleaned = self.clean_text(text)\n",
        "            tokens = self.tokenize(cleaned)\n",
        "            word_freq.update(tokens)\n",
        "\n",
        "        # Add most common words to vocabulary\n",
        "        most_common = word_freq.most_common(max_vocab_size - 2)\n",
        "\n",
        "        for word, freq in most_common:\n",
        "            if word not in self.word2idx:\n",
        "                self.word2idx[word] = self.vocab_size\n",
        "                self.idx2word[self.vocab_size] = word\n",
        "                self.vocab_size += 1\n",
        "\n",
        "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
        "        return self.word2idx\n",
        "\n",
        "    def text_to_sequence(self, text, max_length=150):\n",
        "        \"\"\"Convert text to sequence of indices\"\"\"\n",
        "        cleaned = self.clean_text(text)\n",
        "        tokens = self.tokenize(cleaned)\n",
        "\n",
        "        # Convert to indices\n",
        "        sequence = [self.word2idx.get(token, 1) for token in tokens]\n",
        "\n",
        "        # Pad or truncate\n",
        "        if len(sequence) < max_length:\n",
        "            sequence = sequence + [0] * (max_length - len(sequence))\n",
        "        else:\n",
        "            sequence = sequence[:max_length]\n",
        "\n",
        "        return sequence\n",
        "\n",
        "    def texts_to_sequences(self, texts, max_length=150):\n",
        "        \"\"\"Convert multiple texts to sequences\"\"\"\n",
        "        sequences = []\n",
        "        for text in tqdm(texts, desc=\"Converting texts\"):\n",
        "            sequences.append(self.text_to_sequence(text, max_length))\n",
        "        return np.array(sequences)"
      ],
      "metadata": {
        "id": "A38GfAbYF2qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATASET CLASS WITH AUGMENTATION**"
      ],
      "metadata": {
        "id": "TabKxkBWGa0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, sequences, labels, augment=False):\n",
        "        self.sequences = torch.LongTensor(sequences)\n",
        "        self.labels = torch.FloatTensor(labels)\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = self.sequences[idx]\n",
        "\n",
        "        # Simple sequence augmentation: random dropout\n",
        "        if self.augment and np.random.random() < 0.2:\n",
        "            mask = torch.rand(sequence.shape) > 0.1\n",
        "            sequence = sequence * mask.long()\n",
        "\n",
        "        return {\n",
        "            'sequence': sequence,\n",
        "            'labels': self.labels[idx]\n",
        "        }"
      ],
      "metadata": {
        "id": "4ld8_43vF8Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ATTENTION MECHANISM**"
      ],
      "metadata": {
        "id": "vuZA-GzZGd8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads=4):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "\n",
        "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, hidden_dim = x.size()\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention\n",
        "        attended = torch.matmul(attention_weights, V)\n",
        "        attended = attended.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_dim)\n",
        "\n",
        "        output = self.out(attended)\n",
        "        return output"
      ],
      "metadata": {
        "id": "tO8N3YslF-2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL ARCHITECTURE**"
      ],
      "metadata": {
        "id": "ofhfx4tQGilj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedEmotionClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers,\n",
        "                 num_labels, dropout=0.4, bidirectional=True):\n",
        "        super(EnhancedEmotionClassifier, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # Embedding layer with dropout\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.embedding_dropout = nn.Dropout(0.2)\n",
        "\n",
        "        # GRU layers (often better than LSTM for text)\n",
        "        self.gru = nn.GRU(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        # Multi-head attention\n",
        "        gru_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
        "        self.attention = MultiHeadAttention(gru_output_dim, num_heads=4)\n",
        "\n",
        "        # Global max pooling\n",
        "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
        "\n",
        "        # Feature combination\n",
        "        combined_dim = gru_output_dim * 2  # attention + max pool\n",
        "\n",
        "        # Enhanced fully connected layers\n",
        "        self.fc1 = nn.Linear(combined_dim, hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_dim // 4)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_dim // 4, num_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embedding\n",
        "        embedded = self.embedding(x)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "\n",
        "        # GRU\n",
        "        gru_out, _ = self.gru(embedded)\n",
        "\n",
        "        # Multi-head attention\n",
        "        attended = self.attention(gru_out)\n",
        "\n",
        "        # Mean of attended output\n",
        "        attended_mean = torch.mean(attended, dim=1)\n",
        "\n",
        "        # Max pooling over sequence\n",
        "        max_pooled = self.global_max_pool(gru_out.transpose(1, 2)).squeeze(2)\n",
        "\n",
        "        # Combine features\n",
        "        combined = torch.cat([attended_mean, max_pooled], dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.fc1(combined)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        logits = self.fc_out(x)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "bjAWcjJ0GEqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FOCAL LOSS FOR IMBALANCED DATA**"
      ],
      "metadata": {
        "id": "HydkfaGVGmTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "        return F_loss.mean()"
      ],
      "metadata": {
        "id": "InCuMPouGGlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAINING & EVALUATION FUNCTIONS**"
      ],
      "metadata": {
        "id": "mR04gqBCGpnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    progress_bar = tqdm(data_loader, desc='Training')\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        sequences = batch['sequence'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(sequences)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        progress_bar.set_postfix({'loss': np.mean(losses)})\n",
        "\n",
        "    return np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, criterion, device, threshold=0.5):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc='Evaluating'):\n",
        "            sequences = batch['sequence'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            logits = model(sequences)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs > threshold).int()\n",
        "\n",
        "            all_predictions.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
        "    f1_per_class = f1_score(all_labels, all_predictions, average=None, zero_division=0)\n",
        "\n",
        "    return np.mean(losses), f1_macro, f1_per_class\n",
        "\n",
        "def predict(model, data_loader, device, threshold=0.5):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc='Predicting'):\n",
        "            sequences = batch['sequence'].to(device)\n",
        "\n",
        "            logits = model(sequences)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs > threshold).int()\n",
        "\n",
        "            all_predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "    return np.array(all_predictions)"
      ],
      "metadata": {
        "id": "3iMat14WEjil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREPARE DATA**"
      ],
      "metadata": {
        "id": "v_C1jNl6Guzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize preprocessor\n",
        "preprocessor = EnhancedTextPreprocessor()\n",
        "\n",
        "# Build vocabulary\n",
        "preprocessor.build_vocab(train_df['text'].tolist(), max_vocab_size=Config.VOCAB_SIZE)\n",
        "\n",
        "# Convert texts to sequences\n",
        "print(\"\\nConverting texts to sequences...\")\n",
        "sequences = preprocessor.texts_to_sequences(\n",
        "    train_df['text'].tolist(),\n",
        "    max_length=Config.MAX_LENGTH\n",
        ")\n",
        "\n",
        "labels = train_df[Config.EMOTION_COLS].values\n",
        "\n",
        "# Split data\n",
        "train_seq, val_seq, train_labels, val_labels = train_test_split(\n",
        "    sequences, labels, test_size=0.15, random_state=Config.SEED\n",
        ")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = EmotionDataset(train_seq, train_labels, augment=Config.USE_AUGMENTATION)\n",
        "val_dataset = EmotionDataset(val_seq, val_labels, augment=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE)\n",
        "\n",
        "# INITIALIZE MODEL\n",
        "model = EnhancedEmotionClassifier(\n",
        "    vocab_size=preprocessor.vocab_size,\n",
        "    embedding_dim=Config.EMBEDDING_DIM,\n",
        "    hidden_dim=Config.HIDDEN_DIM,\n",
        "    num_layers=Config.NUM_LAYERS,\n",
        "    num_labels=Config.NUM_LABELS,\n",
        "    dropout=Config.DROPOUT,\n",
        "    bidirectional=Config.BIDIRECTIONAL\n",
        ")\n",
        "model.to(Config.DEVICE)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=Config.LEARNING_RATE,\n",
        "    weight_decay=Config.WEIGHT_DECAY\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer, T_0=5, T_mult=2\n",
        ")"
      ],
      "metadata": {
        "id": "uAwq-TYXEjf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAINING LOOP**"
      ],
      "metadata": {
        "id": "dzjVwed7HH2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "best_f1 = 0\n",
        "patience_counter = 0\n",
        "patience = 5\n",
        "\n",
        "history = {'train_loss': [], 'val_loss': [], 'val_f1': []}\n",
        "\n",
        "for epoch in range(Config.EPOCHS):\n",
        "    print(f'\\nEpoch {epoch + 1}/{Config.EPOCHS}')\n",
        "    print('-' * 60)\n",
        "\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion, Config.DEVICE)\n",
        "    val_loss, val_f1, val_f1_per_class = eval_model(model, val_loader, criterion, Config.DEVICE)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_f1'].append(val_f1)\n",
        "\n",
        "    wandb.log({\n",
        "        \"train_loss\": train_loss,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_f1\": val_f1,\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "    })\n",
        "\n",
        "    print(f'\\nTrain Loss: {train_loss:.4f}')\n",
        "    print(f'Val Loss: {val_loss:.4f}')\n",
        "    print(f'Val Macro F1: {val_f1:.4f}')\n",
        "    print(f'\\nPer-class F1 scores:')\n",
        "    for emotion, f1 in zip(Config.EMOTION_COLS, val_f1_per_class):\n",
        "        wandb.log({f\"F1_{emotion}\": f1})\n",
        "        print(f'  {emotion}: {f1:.4f}')\n",
        "\n",
        "    # Save best model\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        patience_counter = 0\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'preprocessor': preprocessor,\n",
        "            'config': Config\n",
        "        }, 'best_model.pt')\n",
        "        print(f'\\nâœ“ New best model saved! (F1: {best_f1:.4f})')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"TRAINING COMPLETED - Best F1: {best_f1:.4f}\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "id": "M9lIgN-MEvqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "naGParHaEvnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saving and Uploading the Trained Model**"
      ],
      "metadata": {
        "id": "J1wquKSiHTAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'preprocessor': preprocessor,\n",
        "    'config': Config\n",
        "}, 'best_model.pt')\n",
        "\n",
        "user = \"somya2611\"\n",
        "emotion_handle = f\"{user}/emotion-classifier/pyTorch/v1\"\n",
        "os.makedirs(\"emotion_model_dir\", exist_ok=True)\n",
        "os.rename(\"best_model.pt\", \"emotion_model_dir/best_model.pt\")\n",
        "kagglehub.model_upload(emotion_handle, \"emotion_model_dir\")"
      ],
      "metadata": {
        "id": "vQk-7ATtEvkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Downloading the Saved Model from Kaggle**"
      ],
      "metadata": {
        "id": "Cj64lSUlHZTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "user = \"somya2611\"\n",
        "emotion_handle = f\"{user}/emotion-classifier/pyTorch/v1\"\n",
        "\n",
        "model_path = kagglehub.model_download(emotion_handle)\n",
        "\n",
        "checkpoint = torch.load(f\"{model_path}/best_model.pt\", map_location=device, weights_only=False)\n",
        "\n",
        "# Recreate model architecture\n",
        "model = EnhancedEmotionClassifier(\n",
        "    vocab_size=checkpoint['preprocessor'].vocab_size,\n",
        "    embedding_dim=checkpoint['config'].EMBEDDING_DIM,\n",
        "    hidden_dim=checkpoint['config'].HIDDEN_DIM,\n",
        "    num_layers=checkpoint['config'].NUM_LAYERS,\n",
        "    num_labels=checkpoint['config'].NUM_LABELS,\n",
        "    dropout=checkpoint['config'].DROPOUT,\n",
        "    bidirectional=checkpoint['config'].BIDIRECTIONAL\n",
        ")\n",
        "\n",
        "# Load trained weights\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Get preprocessor\n",
        "preprocessor = checkpoint['preprocessor']\n",
        "\n",
        "# Process and predict\n",
        "test_sequences = preprocessor.texts_to_sequences(\n",
        "    test_df['text'].tolist(),\n",
        "    max_length=checkpoint['config'].MAX_LENGTH\n",
        ")\n",
        "\n",
        "# Create dataset and loader\n",
        "test_labels_dummy = torch.zeros((len(test_sequences), 5))\n",
        "test_dataset = EmotionDataset(test_sequences, test_labels_dummy, augment=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "2iEmaN7UE2xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Running Inference and Generating Submission File**"
      ],
      "metadata": {
        "id": "Z3-0-iBoHtay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict(model, test_loader, device, threshold=0.5)\n",
        "\n",
        "submission = pd.DataFrame(predictions, columns=['anger', 'fear', 'joy', 'sadness', 'surprise'])\n",
        "if 'id' in test_df.columns:\n",
        "    submission.insert(0, 'id', test_df['id'])\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(submission.head())"
      ],
      "metadata": {
        "id": "srf73F-IHwXd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}